{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrea-1704/Pytorch_Geometric_tutorial/blob/main/train_model_baseline_f1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Summary\n",
        "In this project we are going to subtitute the graphSAGE with a GAT network in order to introduce a form of self-attention.\n",
        "\n",
        "For this implementation we will rely on the build-in implementation of the GAT layers provided by torch geometric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNziUzq9nTdU",
        "outputId": "6716be7d-c2cb-439e-d8e6-674183246c2a"
      },
      "outputs": [],
      "source": [
        "# !pip install torch-scatter -f https://data.pyg.org/whl/torch-2.4.0+cpu.html\n",
        "# !pip install torch-sparse -f https://data.pyg.org/whl/torch-2.4.0+cpu.html\n",
        "# !pip install torch-cluster -f https://data.pyg.org/whl/torch-2.4.0+cpu.html\n",
        "# !pip install torch-spline-conv -f https://data.pyg.org/whl/torch-2.4.0+cpu.html\n",
        "# !pip install torch-geometric==2.6.0 -f https://data.pyg.org/whl/torch-2.4.0+cpu.html\n",
        "# !pip install pyg-lib -f https://data.pyg.org/whl/torch-2.4.0+cpu.html\n",
        "\n",
        "# !pip install pytorch_frame[full]==1.2.2\n",
        "# !pip install relbench[full]==1.0.0\n",
        "# !pip uninstall -y pyg_lib torch  # Uninstall current versions\n",
        "# !pip install torch==2.6.0  # Reinstall your desired PyTorch version\n",
        "# !pip install --no-cache-dir git+https://github.com/pyg-team/pyg-lib.git # Install pyg-lib; --no-cache-dir ensures a fresh install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6DWB-Kf6nl2y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import relbench\n",
        "import numpy as np\n",
        "\n",
        "from torch.nn import BCEWithLogitsLoss, L1Loss\n",
        "from relbench.datasets import get_dataset\n",
        "from relbench.tasks import get_task\n",
        "\n",
        "dataset = get_dataset(\"rel-f1\", download=True)\n",
        "task = get_task(\"rel-f1\", \"driver-position\", download=True)\n",
        "\n",
        "train_table = task.get_table(\"train\")\n",
        "val_table = task.get_table(\"val\")\n",
        "test_table = task.get_table(\"test\")\n",
        "\n",
        "out_channels = 1\n",
        "loss_fn = L1Loss()\n",
        "tune_metric = \"mae\"\n",
        "higher_is_better = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKFT5H51j_Um"
      },
      "source": [
        "Let's check out the training table just to make sure it looks fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABN_fdN3kAB9",
        "outputId": "95345755-7493-43f4-92cb-e6107cfae269"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Table(df=\n",
              "           date  driverId  position\n",
              "0    2004-07-05        10     10.75\n",
              "1    2004-07-05        47     12.00\n",
              "2    2004-03-07         7     15.00\n",
              "3    2004-01-07        10      9.00\n",
              "4    2003-09-09        52     13.00\n",
              "...         ...       ...       ...\n",
              "7448 1995-08-22        96     15.75\n",
              "7449 1975-06-08       228      8.00\n",
              "7450 1965-05-31       418     16.00\n",
              "7451 1961-08-20       467     37.00\n",
              "7452 1954-05-29       677     30.00\n",
              "\n",
              "[7453 rows x 3 columns],\n",
              "  fkey_col_to_pkey_table={'driverId': 'drivers'},\n",
              "  pkey_col=None,\n",
              "  time_col=date)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQhuHIdHkOxv"
      },
      "source": [
        "Note that to load the data we did not require any deep learning libraries. Now we introduce the PyTorch Frame library, which is useful for encoding individual tables into initial node features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNzfdwsrkPIo",
        "outputId": "e68ea332-481b-4a28-d807-12d0721b8925"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\andrea\\Desktop\\Tesi (GNN)\\Laboratori GNNs\\venv\\lib\\site-packages\\torch_geometric\\typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: [WinError 127] Impossibile trovare la procedura specificata\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "c:\\Users\\andrea\\Desktop\\Tesi (GNN)\\Laboratori GNNs\\venv\\lib\\site-packages\\torch_geometric\\typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: [WinError 127] Impossibile trovare la procedura specificata\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
            "c:\\Users\\andrea\\Desktop\\Tesi (GNN)\\Laboratori GNNs\\venv\\lib\\site-packages\\torch_geometric\\typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: [WinError 127] Impossibile trovare la procedura specificata\n",
            "  warnings.warn(\n",
            "c:\\Users\\andrea\\Desktop\\Tesi (GNN)\\Laboratori GNNs\\venv\\lib\\site-packages\\torch_geometric\\typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: [WinError 127] Impossibile trovare la procedura specificata\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n",
            "c:\\Users\\andrea\\Desktop\\Tesi (GNN)\\Laboratori GNNs\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch_geometric\n",
        "import torch_frame\n",
        "\n",
        "# Some book keeping\n",
        "from torch_geometric.seed import seed_everything\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)  # check that it's cuda if you want it to run in reasonable time!\n",
        "root_dir = \"./data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y79g5H0kVjX"
      },
      "source": [
        "The first big move is to build a graph out of the database. Here we use our pre-prepared conversion function.\n",
        "\n",
        "The source code can be found at: https://github.com/snap-stanford/relbench/blob/main/relbench/modeling/graph.py\n",
        "\n",
        "Each node in the graph corresonds to a single row in the database. Crucially, PyTorch Frame stores whole tables as objects in a way that is compatibile with PyG minibatch sampling, meaning we can sample subgraphs as in https://arxiv.org/abs/1706.02216, and retrieve the relevant raw features.\n",
        "\n",
        "PyTorch Frame also stores the `stype` (i.e., modality) of each column, and any specialized feature encoders (e.g., text encoders) to be used later. So we need to configure the `stype` for each column, for which we use a function that tries to automatically detect the `stype`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiV3TGI-kRuy",
        "outputId": "9d4686c6-baf3-48f8-b084-25f28cff0858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Database object from C:\\Users\\andrea\\AppData\\Local\\relbench\\relbench\\Cache/rel-f1/db...\n",
            "Done in 0.05 seconds.\n"
          ]
        }
      ],
      "source": [
        "from relbench.modeling.utils import get_stype_proposal\n",
        "\n",
        "db = dataset.get_db()\n",
        "col_to_stype_dict = get_stype_proposal(db)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm3uYXqXkbZt"
      },
      "source": [
        "If trying a new dataset, you should definitely check through this dict of `stype`s to check that look right, and manually change any mistakes by the auto-detection function.\n",
        "\n",
        "Next we also define our text encoding model, which we use GloVe embeddings for speed and convenience. Feel free to try alternatives here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QQHYmgIxkX1j"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from typing import List, Optional\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# from torch import Tensor\n",
        "\n",
        "\n",
        "# class GloveTextEmbedding:\n",
        "#     def __init__(self, device: Optional[torch.device\n",
        "#                                        ] = None):\n",
        "#         self.model = SentenceTransformer(\n",
        "#             \"sentence-transformers/average_word_embeddings_glove.6B.300d\",\n",
        "#             device=device,\n",
        "#         )\n",
        "\n",
        "#     def __call__(self, sentences: List[str]) -> Tensor:\n",
        "#         return torch.from_numpy(self.model.encode(sentences))\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "class LightweightGloveEmbedder:\n",
        "    def __init__(self, device=None):\n",
        "        self.device = device\n",
        "        self.embeddings = defaultdict(lambda: np.zeros(300))\n",
        "        self._load_embeddings()\n",
        "\n",
        "    def _load_embeddings(self):\n",
        "        try:\n",
        "            #(senza bisogno di estrarre zip)\n",
        "            url = \"https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.300d.txt\"\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            for line in StringIO(response.text):\n",
        "                parts = line.split()\n",
        "                word = parts[0]\n",
        "                vector = np.array(parts[1:], dtype=np.float32)\n",
        "                self.embeddings[word] = vector\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Couldn't load GloVe embeddings ({str(e)}). Using zero vectors.\")\n",
        "\n",
        "    def __call__(self, sentences):\n",
        "        results = []\n",
        "        for text in sentences:\n",
        "            words = text.lower().split()\n",
        "            vectors = [self.embeddings[w] for w in words if w in self.embeddings]\n",
        "            if vectors:\n",
        "                avg_vector = np.mean(vectors, axis=0)\n",
        "            else:\n",
        "                avg_vector = np.zeros(300)\n",
        "            results.append(avg_vector)\n",
        "\n",
        "        tensor = torch.tensor(np.array(results), dtype=torch.float32)\n",
        "        return tensor.to(self.device) if self.device else tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-BBpUrakdwY",
        "outputId": "520fd668-67b2-455a-86ea-c18a1643e095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Couldn't load GloVe embeddings (404 Client Error: Not Found for url: https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.300d.txt). Using zero vectors.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\andrea\\Desktop\\Tesi (GNN)\\Laboratori GNNs\\venv\\lib\\site-packages\\torch_frame\\utils\\io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "c:\\Users\\andrea\\Desktop\\Tesi (GNN)\\Laboratori GNNs\\venv\\lib\\site-packages\\torch_frame\\utils\\io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "c:\\Users\\andrea\\Desktop\\Tesi (GNN)\\Laboratori GNNs\\venv\\lib\\site-packages\\torch_frame\\utils\\io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "c:\\Users\\andrea\\Desktop\\Tesi (GNN)\\Laboratori GNNs\\venv\\lib\\site-packages\\torch_frame\\utils\\io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "c:\\Users\\andrea\\Desktop\\Tesi (GNN)\\Laboratori GNNs\\venv\\lib\\site-packages\\torch_frame\\utils\\io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "c:\\Users\\andrea\\Desktop\\Tesi (GNN)\\Laboratori GNNs\\venv\\lib\\site-packages\\torch_frame\\utils\\io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "c:\\Users\\andrea\\Desktop\\Tesi (GNN)\\Laboratori GNNs\\venv\\lib\\site-packages\\torch_frame\\utils\\io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n"
          ]
        }
      ],
      "source": [
        "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
        "from relbench.modeling.graph import make_pkey_fkey_graph\n",
        "\n",
        "text_embedder_cfg = TextEmbedderConfig(\n",
        "    text_embedder=LightweightGloveEmbedder(device=device), batch_size=256\n",
        ")\n",
        "\n",
        "data, col_stats_dict = make_pkey_fkey_graph(\n",
        "    #Solution if not working: !pip install --upgrade torch torchvision transformers\n",
        "    db,\n",
        "    col_to_stype_dict=col_to_stype_dict,  # speficied column types\n",
        "    text_embedder_cfg=text_embedder_cfg,  # our chosen text encoder\n",
        "    cache_dir=os.path.join(\n",
        "        root_dir, f\"rel-f1_materialized_cache\"\n",
        "    ),  # store materialized graph for convenience\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwQejmg0kzOg"
      },
      "source": [
        "We can now check out `data`, our main graph object. `data` is a heterogeneous and temporal graph, with node types given by the table it originates from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gt4a8lw1kufy",
        "outputId": "248afeff-dc57-43e7-a2b4-00e9eb49ed49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "HeteroData(\n",
              "  circuits={ tf=TensorFrame([77, 7]) },\n",
              "  constructors={ tf=TensorFrame([211, 3]) },\n",
              "  constructor_results={\n",
              "    tf=TensorFrame([9408, 2]),\n",
              "    time=[9408],\n",
              "  },\n",
              "  constructor_standings={\n",
              "    tf=TensorFrame([10170, 4]),\n",
              "    time=[10170],\n",
              "  },\n",
              "  drivers={ tf=TensorFrame([857, 6]) },\n",
              "  qualifying={\n",
              "    tf=TensorFrame([4082, 3]),\n",
              "    time=[4082],\n",
              "  },\n",
              "  races={\n",
              "    tf=TensorFrame([820, 5]),\n",
              "    time=[820],\n",
              "  },\n",
              "  results={\n",
              "    tf=TensorFrame([20323, 11]),\n",
              "    time=[20323],\n",
              "  },\n",
              "  standings={\n",
              "    tf=TensorFrame([28115, 4]),\n",
              "    time=[28115],\n",
              "  },\n",
              "  (constructor_results, f2p_raceId, races)={ edge_index=[2, 9408] },\n",
              "  (races, rev_f2p_raceId, constructor_results)={ edge_index=[2, 9408] },\n",
              "  (constructor_results, f2p_constructorId, constructors)={ edge_index=[2, 9408] },\n",
              "  (constructors, rev_f2p_constructorId, constructor_results)={ edge_index=[2, 9408] },\n",
              "  (constructor_standings, f2p_raceId, races)={ edge_index=[2, 10170] },\n",
              "  (races, rev_f2p_raceId, constructor_standings)={ edge_index=[2, 10170] },\n",
              "  (constructor_standings, f2p_constructorId, constructors)={ edge_index=[2, 10170] },\n",
              "  (constructors, rev_f2p_constructorId, constructor_standings)={ edge_index=[2, 10170] },\n",
              "  (qualifying, f2p_raceId, races)={ edge_index=[2, 4082] },\n",
              "  (races, rev_f2p_raceId, qualifying)={ edge_index=[2, 4082] },\n",
              "  (qualifying, f2p_driverId, drivers)={ edge_index=[2, 4082] },\n",
              "  (drivers, rev_f2p_driverId, qualifying)={ edge_index=[2, 4082] },\n",
              "  (qualifying, f2p_constructorId, constructors)={ edge_index=[2, 4082] },\n",
              "  (constructors, rev_f2p_constructorId, qualifying)={ edge_index=[2, 4082] },\n",
              "  (races, f2p_circuitId, circuits)={ edge_index=[2, 820] },\n",
              "  (circuits, rev_f2p_circuitId, races)={ edge_index=[2, 820] },\n",
              "  (results, f2p_raceId, races)={ edge_index=[2, 20323] },\n",
              "  (races, rev_f2p_raceId, results)={ edge_index=[2, 20323] },\n",
              "  (results, f2p_driverId, drivers)={ edge_index=[2, 20323] },\n",
              "  (drivers, rev_f2p_driverId, results)={ edge_index=[2, 20323] },\n",
              "  (results, f2p_constructorId, constructors)={ edge_index=[2, 20323] },\n",
              "  (constructors, rev_f2p_constructorId, results)={ edge_index=[2, 20323] },\n",
              "  (standings, f2p_raceId, races)={ edge_index=[2, 28115] },\n",
              "  (races, rev_f2p_raceId, standings)={ edge_index=[2, 28115] },\n",
              "  (standings, f2p_driverId, drivers)={ edge_index=[2, 28115] },\n",
              "  (drivers, rev_f2p_driverId, standings)={ edge_index=[2, 28115] }\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd6DqCXgk41x"
      },
      "source": [
        "We can also check out the TensorFrame for one table like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mMQTQeLk1rl",
        "outputId": "b106d0e6-9eb1-4999-c283-fa957e9f5efa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorFrame(\n",
              "  num_cols=5,\n",
              "  num_rows=820,\n",
              "  categorical (1): ['year'],\n",
              "  numerical (1): ['round'],\n",
              "  timestamp (2): ['date', 'time'],\n",
              "  embedding (1): ['name'],\n",
              "  has_target=False,\n",
              "  device='cpu',\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[\"races\"].tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kbysKXMk-3X"
      },
      "source": [
        "This may be a little confusing at first, as in graph ML it is more standard to associate to the graph object `data` a tensor, e.g., `data.x` for which `data.x[idx]` is a 1D array/tensor storing all the features for node with index `idx`.\n",
        "\n",
        "But actually this `data` object behaves similarly. For a given node type, e.g., `races` again, `data['races']` stores two pieces of information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDIcp7L5k6pU",
        "outputId": "0cef0148-aed1-4ff0-f2c0-90f60f2257b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tf', 'time']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(data[\"races\"].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z18qPRPllB1H"
      },
      "source": [
        "A `TensorFrame` object, and a timestamp for each node. The `TensorFrame` object acts analogously to the usual tensor of node features, and you can simply use indexing to retrieve the features of a single row (node), or group of nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im8bhNh5lFG6",
        "outputId": "d2d86c31-82d5-4cb7-bc73-0cce649672db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorFrame(\n",
              "  num_cols=5,\n",
              "  num_rows=1,\n",
              "  categorical (1): ['year'],\n",
              "  numerical (1): ['round'],\n",
              "  timestamp (2): ['date', 'time'],\n",
              "  embedding (1): ['name'],\n",
              "  has_target=False,\n",
              "  device='cpu',\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[\"races\"].tf[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYZ28pzNlG4s",
        "outputId": "e3b1dc23-8dff-4c62-8297-fddb0e0961d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorFrame(\n",
              "  num_cols=5,\n",
              "  num_rows=10,\n",
              "  categorical (1): ['year'],\n",
              "  numerical (1): ['round'],\n",
              "  timestamp (2): ['date', 'time'],\n",
              "  embedding (1): ['name'],\n",
              "  has_target=False,\n",
              "  device='cpu',\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[\"races\"].tf[10:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql15svcelK3A"
      },
      "source": [
        "We can also check the edge indices between two different node types, such as `races` amd `circuits`. Note that the edges are also heterogenous, so we also need to specify which edge type we want to look at. Here we look at `f2p_curcuitId`, which are the directed edges pointing _from_ a race (the `f` stands for `foreign key`), _to_ the circuit at which te race happened (the `p` stands for `primary key`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TynkD36QlInL",
        "outputId": "50bf70ff-2980-43e2-c53e-020acb0480f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'edge_index': tensor([[  0,   1,   2,  ..., 817, 818, 819],\n",
              "        [  8,   5,  18,  ...,  21,  17,  23]])}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[(\"races\", \"f2p_circuitId\", \"circuits\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx4V5KCelNxl"
      },
      "source": [
        "Now we are ready to instantiate our data loaders. For this we will need to import PyTorch Geometric, our GNN library. Whilst we're at it let's add a seed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HUHVG-g6lM-b"
      },
      "outputs": [],
      "source": [
        "from relbench.modeling.graph import get_node_train_table_input, make_pkey_fkey_graph\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "\n",
        "loader_dict = {}\n",
        "\n",
        "for split, table in [\n",
        "    (\"train\", train_table),\n",
        "    (\"val\", val_table),\n",
        "    (\"test\", test_table),\n",
        "]:\n",
        "    table_input = get_node_train_table_input(\n",
        "        table=table,\n",
        "        task=task,\n",
        "    )\n",
        "    entity_table = table_input.nodes[0]\n",
        "    loader_dict[split] = NeighborLoader(\n",
        "        data,\n",
        "        num_neighbors=[\n",
        "            128 for i in range(2)\n",
        "        ],  # we sample subgraphs of depth 2, 128 neighbors per node.\n",
        "        time_attr=\"time\",\n",
        "        input_nodes=table_input.nodes,\n",
        "        input_time=table_input.time,\n",
        "        transform=table_input.transform,\n",
        "        batch_size=512,\n",
        "        temporal_strategy=\"uniform\",\n",
        "        shuffle=split == \"train\",\n",
        "        num_workers=0,\n",
        "        persistent_workers=False,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQc8BWsGludR"
      },
      "source": [
        "Now we need our model...\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch_geometric.nn import HeteroConv, GATConv\n",
        "\n",
        "class HeteroGAT(torch.nn.Module):\n",
        "    def __init__(self, node_types, edge_types, channels, heads=2, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.edge_types = edge_types\n",
        "        #per ogni layer andiamoa a creare un HeteroConv che contiene \n",
        "        #tanti GATConv (che sono già implementati) quanti sono gli \n",
        "        #edge type.\n",
        "        for i in range(num_layers):\n",
        "            conv = HeteroConv(\n",
        "                {\n",
        "                    edge_type: GATConv(\n",
        "                        (-1, -1), channels, heads=heads, concat=False, add_self_loops=False\n",
        "                    )\n",
        "                    for edge_type in edge_types\n",
        "                },\n",
        "                aggr=\"sum\",\n",
        "            )\n",
        "            self.convs.append(conv)\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict, *args, **kwargs):\n",
        "        for conv in self.convs:\n",
        "            x_dict = conv(x_dict, edge_index_dict)\n",
        "        return x_dict\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            for edge_type in self.edge_types:\n",
        "                conv.convs[edge_type].reset_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How does it works?\n",
        "\n",
        "It's pretty simple: for each layer of the network we are creating an HeteroConv layer, which can be seen as a wrapper of GNN layers. Then, for each edge_type in that layer we are creating a GAT layer.\n",
        "\n",
        "The reason why we need a different GAT layer for each different edge type of a given layer is that each layer requires to be considered separately for it's semantic difference with all the others. \n",
        "\n",
        "So each of the edge type requires its own attention scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NB:\n",
        "\n",
        "When we do something like:\n",
        "~~~python\n",
        "edge_type: GATConv(\n",
        "    (-1, -1), channels, heads=heads, concat=False, add_self_loops=False\n",
        ")\n",
        "~~~\n",
        "\n",
        "The \"(-1, -1)\" refers to the dimension of the input channels. In this case we are using 'lazy initialization': PyTorch Geometric will automatically infer the in_channels during the first forward() call by reading the actual dimensions of the source and destination node embeddings.\n",
        "\n",
        "This is especially useful in heterogeneous graphs, where dimensions may vary across different types (or be decided dynamically)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "u3m3jEqClQnw"
      },
      "outputs": [],
      "source": [
        "from torch.nn import BCEWithLogitsLoss\n",
        "import copy\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.nn import Embedding, ModuleDict\n",
        "from torch_frame.data.stats import StatType\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import MLP\n",
        "from torch_geometric.typing import NodeType\n",
        "\n",
        "from relbench.modeling.nn import HeteroEncoder, HeteroGraphSAGE, HeteroTemporalEncoder\n",
        "\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data: HeteroData,\n",
        "        col_stats_dict: Dict[str, Dict[str, Dict[StatType, Any]]],\n",
        "        num_layers: int,\n",
        "        channels: int,\n",
        "        out_channels: int,\n",
        "        aggr: str,\n",
        "        norm: str,\n",
        "        # List of node types to add shallow embeddings to input\n",
        "        shallow_list: List[NodeType] = [],\n",
        "        # ID awareness\n",
        "        id_awareness: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = HeteroEncoder(\n",
        "            channels=channels,\n",
        "            #channels specify the dimension of the embedding produced by the encoder\n",
        "            node_to_col_names_dict={\n",
        "                node_type: data[node_type].tf.col_names_dict\n",
        "                for node_type in data.node_types\n",
        "            },\n",
        "            #creiamo un dizionario che contiene per ogni tipo di nodo\n",
        "            #i nomi delle colonne della tabella che lo riguardano.\n",
        "            node_to_col_stats=col_stats_dict,\n",
        "            #col_stats_dict è un dizionario che contiene per ogni colonna\n",
        "            #le statistiche (mean, std, min, max) delle colonne\n",
        "        )\n",
        "        self.temporal_encoder = HeteroTemporalEncoder(\n",
        "            node_types=[\n",
        "                node_type for node_type in data.node_types if \"time\" in data[node_type]\n",
        "            ],\n",
        "            channels=channels,\n",
        "        )\n",
        "        self.gnn = HeteroGAT(\n",
        "            node_types=data.node_types,\n",
        "            edge_types=data.edge_types,\n",
        "            channels=channels,\n",
        "            heads=4,\n",
        "            num_layers=num_layers,\n",
        "        )\n",
        "        self.head = MLP(\n",
        "            channels,\n",
        "            out_channels=out_channels,\n",
        "            norm=norm,\n",
        "            num_layers=1,\n",
        "        )\n",
        "        self.embedding_dict = ModuleDict(\n",
        "            {\n",
        "                node: Embedding(data.num_nodes_dict[node], channels)\n",
        "                for node in shallow_list\n",
        "            }\n",
        "        )\n",
        "\n",
        "        self.id_awareness_emb = None\n",
        "        if id_awareness:\n",
        "            self.id_awareness_emb = torch.nn.Embedding(1, channels)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.encoder.reset_parameters()\n",
        "        self.temporal_encoder.reset_parameters()\n",
        "        self.gnn.reset_parameters()\n",
        "        self.head.reset_parameters()\n",
        "        for embedding in self.embedding_dict.values():\n",
        "            torch.nn.init.normal_(embedding.weight, std=0.1)\n",
        "        if self.id_awareness_emb is not None:\n",
        "            self.id_awareness_emb.reset_parameters()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        batch: HeteroData,\n",
        "        entity_table: NodeType,\n",
        "    ) -> Tensor:\n",
        "        seed_time = batch[entity_table].seed_time\n",
        "        x_dict = self.encoder(batch.tf_dict)\n",
        "\n",
        "        rel_time_dict = self.temporal_encoder(\n",
        "            seed_time, batch.time_dict, batch.batch_dict\n",
        "        )\n",
        "\n",
        "        for node_type, rel_time in rel_time_dict.items():\n",
        "            x_dict[node_type] = x_dict[node_type] + rel_time\n",
        "\n",
        "        for node_type, embedding in self.embedding_dict.items():\n",
        "            x_dict[node_type] = x_dict[node_type] + embedding(batch[node_type].n_id)\n",
        "\n",
        "        x_dict = self.gnn(\n",
        "            x_dict,\n",
        "            batch.edge_index_dict,\n",
        "            batch.num_sampled_nodes_dict,\n",
        "            batch.num_sampled_edges_dict,\n",
        "        )\n",
        "\n",
        "        return self.head(x_dict[entity_table][: seed_time.size(0)])\n",
        "\n",
        "    def forward_dst_readout(\n",
        "        self,\n",
        "        batch: HeteroData,\n",
        "        entity_table: NodeType,\n",
        "        dst_table: NodeType,\n",
        "    ) -> Tensor:\n",
        "        if self.id_awareness_emb is None:\n",
        "            raise RuntimeError(\n",
        "                \"id_awareness must be set True to use forward_dst_readout\"\n",
        "            )\n",
        "        seed_time = batch[entity_table].seed_time\n",
        "        x_dict = self.encoder(batch.tf_dict)\n",
        "        # Add ID-awareness to the root node\n",
        "        x_dict[entity_table][: seed_time.size(0)] += self.id_awareness_emb.weight\n",
        "\n",
        "        rel_time_dict = self.temporal_encoder(\n",
        "            seed_time, batch.time_dict, batch.batch_dict\n",
        "        )\n",
        "\n",
        "        for node_type, rel_time in rel_time_dict.items():\n",
        "            x_dict[node_type] = x_dict[node_type] + rel_time\n",
        "\n",
        "        for node_type, embedding in self.embedding_dict.items():\n",
        "            x_dict[node_type] = x_dict[node_type] + embedding(batch[node_type].n_id)\n",
        "\n",
        "        x_dict = self.gnn(\n",
        "            x_dict,\n",
        "            batch.edge_index_dict,\n",
        "        )\n",
        "\n",
        "        return self.head(x_dict[dst_table])\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    data=data,\n",
        "    col_stats_dict=col_stats_dict,\n",
        "    num_layers=2,\n",
        "    channels=128,\n",
        "    out_channels=1,\n",
        "    aggr=\"sum\",\n",
        "    norm=\"batch_norm\",\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# if you try out different RelBench tasks you will need to change these\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "epochs = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl-6So7Llb-p"
      },
      "source": [
        "We also need standard train/test loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SAHRIr15lVs6"
      },
      "outputs": [],
      "source": [
        "def train() -> float:\n",
        "    model.train()\n",
        "\n",
        "    loss_accum = count_accum = 0\n",
        "    for batch in tqdm(loader_dict[\"train\"]):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(\n",
        "            batch,\n",
        "            task.entity_table,\n",
        "        )\n",
        "        pred = pred.view(-1) if pred.size(1) == 1 else pred\n",
        "\n",
        "        loss = loss_fn(pred.float(), batch[entity_table].y.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_accum += loss.detach().item() * pred.size(0)\n",
        "        count_accum += pred.size(0)\n",
        "\n",
        "    return loss_accum / count_accum\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(loader: NeighborLoader) -> np.ndarray:\n",
        "    model.eval()\n",
        "\n",
        "    pred_list = []\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        pred = model(\n",
        "            batch,\n",
        "            task.entity_table,\n",
        "        )\n",
        "        pred = pred.view(-1) if pred.size(1) == 1 else pred\n",
        "        pred_list.append(pred.detach().cpu())\n",
        "    return torch.cat(pred_list, dim=0).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s-p7dW1ledd"
      },
      "source": [
        "Now we are ready to train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Uzkt7AS2GzpH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def rmse(true, pred):\n",
        "    \"\"\"Calculate the Root Mean Squared Error (RMSE).\"\"\"\n",
        "    return np.sqrt(np.mean((true - pred)**2)) # Calculate RMSE manually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qZOOyAblHwI4"
      },
      "outputs": [],
      "source": [
        "def custom_evaluate(pred: np.ndarray, target_table, metrics) -> dict:\n",
        "    \"\"\"Custom evaluation function to replace task.evaluate.\"\"\"\n",
        "\n",
        "    # Extract target values from the target table\n",
        "    target = target_table.df[task.target_col].to_numpy()\n",
        "\n",
        "    # Check for length mismatch\n",
        "    if len(pred) != len(target):\n",
        "        raise ValueError(\n",
        "            f\"The length of pred and target must be the same (got \"\n",
        "            f\"{len(pred)} and {len(target)}, respectively).\"\n",
        "        )\n",
        "\n",
        "    # Calculate metrics\n",
        "    results = {}\n",
        "    for metric_fn in metrics:\n",
        "        if metric_fn.__name__ == \"rmse\":  # Handle RMSE specifically\n",
        "            results[\"rmse\"] = np.sqrt(np.mean((target - pred)**2))\n",
        "        else:  # Handle other metrics (if any)\n",
        "            results[metric_fn.__name__] = metric_fn(target, pred)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        },
        "id": "yF3W68Eqlew_",
        "outputId": "91ae72b2-0964-4a72-f100-acebbec7c95f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [02:30<00:00, 10.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01, Train loss: 8.604311272743868, Val metrics: {'r2': -0.34352017339584484, 'mae': 4.201821075635667, 'rmse': 5.373687235078148}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:57<00:00,  7.84s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 02, Train loss: 6.49869991829935, Val metrics: {'r2': -0.32141643560316835, 'mae': 4.087943496971665, 'rmse': 5.3292996717217305}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:56<00:00,  7.77s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 03, Train loss: 6.213251046905354, Val metrics: {'r2': -0.41817185863939077, 'mae': 4.224287710081519, 'rmse': 5.520961533280505}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:58<00:00,  7.91s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 04, Train loss: 6.086786003220278, Val metrics: {'r2': -0.30090665332157984, 'mae': 4.144229199747762, 'rmse': 5.287779746422875}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:55<00:00,  7.71s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 05, Train loss: 6.024802099821889, Val metrics: {'r2': -0.38212566232212386, 'mae': 4.385383532631772, 'rmse': 5.450345772375066}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:52<00:00,  7.53s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 06, Train loss: 6.069389270453361, Val metrics: {'r2': -0.4816069518869912, 'mae': 4.41686759463293, 'rmse': 5.643087614624379}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:58<00:00,  7.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 07, Train loss: 5.954816577291546, Val metrics: {'r2': -0.27615005465995046, 'mae': 4.242782009993701, 'rmse': 5.237224143929374}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [02:01<00:00,  8.12s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 08, Train loss: 5.788725596601557, Val metrics: {'r2': -0.1589495951319646, 'mae': 3.990524343784921, 'rmse': 4.990942468219732}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [02:08<00:00,  8.60s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 09, Train loss: 5.588443381377199, Val metrics: {'r2': -0.1282179370485974, 'mae': 4.0033533394456144, 'rmse': 4.924325924331741}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:54<00:00,  7.66s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10, Train loss: 5.424774284444687, Val metrics: {'r2': -0.03270222146581481, 'mae': 3.8215846560522166, 'rmse': 4.711268385224534}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:58<00:00,  7.92s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 11, Train loss: 5.272996831256944, Val metrics: {'r2': -0.024076940951798464, 'mae': 3.8488418178392716, 'rmse': 4.691552528981604}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:58<00:00,  7.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 12, Train loss: 5.0892561974340556, Val metrics: {'r2': -0.030201272318498784, 'mae': 3.8330729831754167, 'rmse': 4.70556016424961}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:54<00:00,  7.66s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 13, Train loss: 4.959764454295359, Val metrics: {'r2': -0.013033866983407139, 'mae': 3.783788152018148, 'rmse': 4.666188424014493}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:58<00:00,  7.88s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 14, Train loss: 4.933166029948157, Val metrics: {'r2': 0.027347713150355335, 'mae': 3.6871341354940914, 'rmse': 4.572240810682451}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [02:00<00:00,  8.06s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 15, Train loss: 4.818985963337984, Val metrics: {'r2': -0.022230536298368797, 'mae': 3.8629531757785385, 'rmse': 4.687321200157686}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:54<00:00,  7.66s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 16, Train loss: 4.810262059583194, Val metrics: {'r2': -0.04092250892646421, 'mae': 3.8132847581454414, 'rmse': 4.729982016496382}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:57<00:00,  7.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 17, Train loss: 4.795970491286206, Val metrics: {'r2': -0.0838103917680908, 'mae': 3.937649260159724, 'rmse': 4.82644037124403}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:56<00:00,  7.74s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 18, Train loss: 4.767593986921209, Val metrics: {'r2': -0.023721620177095115, 'mae': 3.849497556144903, 'rmse': 4.690738551712634}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [02:04<00:00,  8.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 19, Train loss: 4.731486701427906, Val metrics: {'r2': -0.23861272726335092, 'mae': 4.243489107388055, 'rmse': 5.159624054005692}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [02:02<00:00,  8.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 20, Train loss: 4.839727796997046, Val metrics: {'r2': -0.19717949386972644, 'mae': 4.157021612044407, 'rmse': 5.072591904869826}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [02:13<00:00,  8.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 21, Train loss: 4.731061951803486, Val metrics: {'r2': -0.10491843500304632, 'mae': 3.9366060583131186, 'rmse': 4.8732130600874095}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [02:10<00:00,  8.68s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 22, Train loss: 4.76778609912923, Val metrics: {'r2': -0.08279454289687638, 'mae': 3.87996368573838, 'rmse': 4.824177944415672}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:54<00:00,  7.66s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 23, Train loss: 4.696444874175776, Val metrics: {'r2': -0.6507840173230601, 'mae': 4.756317650419756, 'rmse': 5.956558581698563}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:57<00:00,  7.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 24, Train loss: 4.671206196698345, Val metrics: {'r2': -0.009778145397757454, 'mae': 3.826542772638376, 'rmse': 4.658684214871621}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:51<00:00,  7.46s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 25, Train loss: 4.677285427281348, Val metrics: {'r2': -0.08214856604808451, 'mae': 3.933183254873904, 'rmse': 4.822738718382585}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:53<00:00,  7.56s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 26, Train loss: 4.615157009274723, Val metrics: {'r2': -0.05789476318271447, 'mae': 3.854514265697482, 'rmse': 4.7683873071524845}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:58<00:00,  7.88s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 27, Train loss: 4.677858306915987, Val metrics: {'r2': -0.2027588634854618, 'mae': 4.053062809668307, 'rmse': 5.08439839136623}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:52<00:00,  7.52s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 28, Train loss: 4.5986685287739695, Val metrics: {'r2': -0.09749001966086768, 'mae': 3.976865484193714, 'rmse': 4.856804020797919}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:55<00:00,  7.69s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 29, Train loss: 4.591272887776686, Val metrics: {'r2': -0.17882265731977598, 'mae': 4.022909653338099, 'rmse': 5.033551621754765}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:56<00:00,  7.75s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 30, Train loss: 4.6130693263692795, Val metrics: {'r2': -0.08470575295892191, 'mae': 3.930940080859618, 'rmse': 4.8284335774798945}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:55<00:00,  7.70s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 31, Train loss: 4.597959658994973, Val metrics: {'r2': -0.3690507151838822, 'mae': 4.428603972668798, 'rmse': 5.424504300725776}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:54<00:00,  7.61s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 32, Train loss: 4.5806340924304845, Val metrics: {'r2': -0.014125061757624957, 'mae': 3.7568368297301693, 'rmse': 4.66870085236674}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:59<00:00,  7.97s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 33, Train loss: 4.539253600184652, Val metrics: {'r2': -0.1281541549412153, 'mae': 3.9378910724051255, 'rmse': 4.924186727677546}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:57<00:00,  7.82s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 34, Train loss: 4.4850429953080315, Val metrics: {'r2': -0.026218911287802626, 'mae': 3.8226464831518507, 'rmse': 4.696456416911022}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:53<00:00,  7.55s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 35, Train loss: 4.4942018245828566, Val metrics: {'r2': -0.26555586795999386, 'mae': 4.237739393439544, 'rmse': 5.21543996444069}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [02:06<00:00,  8.40s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 36, Train loss: 4.440227341303229, Val metrics: {'r2': -0.2005489458530667, 'mae': 4.149487824303035, 'rmse': 5.079725273597906}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:57<00:00,  7.83s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 37, Train loss: 4.482653397322501, Val metrics: {'r2': -0.1565445813066384, 'mae': 4.006794957280079, 'rmse': 4.985761259579259}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:55<00:00,  7.67s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 38, Train loss: 4.437607138457273, Val metrics: {'r2': -0.045937298871707055, 'mae': 3.8904752222951764, 'rmse': 4.741362002143624}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:52<00:00,  7.49s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 39, Train loss: 4.465969165347493, Val metrics: {'r2': -0.06464868363584242, 'mae': 3.9257200432205965, 'rmse': 4.783584503982783}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:56<00:00,  7.79s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 40, Train loss: 4.501164634036135, Val metrics: {'r2': -0.6134596197355919, 'mae': 4.8341014774943005, 'rmse': 5.888834377007195}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:56<00:00,  7.78s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 41, Train loss: 4.516997065429606, Val metrics: {'r2': -0.14704761229031327, 'mae': 4.0974044931834745, 'rmse': 4.965248767018642}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:55<00:00,  7.70s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 42, Train loss: 4.4170192542946864, Val metrics: {'r2': -0.1533342221427676, 'mae': 4.088943001438796, 'rmse': 4.978836663869209}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [02:00<00:00,  8.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 43, Train loss: 4.4481532749322925, Val metrics: {'r2': -0.2551204846461972, 'mae': 4.253439211483071, 'rmse': 5.1938930004976}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:59<00:00,  8.00s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 44, Train loss: 4.44478168952038, Val metrics: {'r2': -0.1666023640054768, 'mae': 4.1128898344122735, 'rmse': 5.007393436051081}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:56<00:00,  7.76s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 45, Train loss: 4.283127451141329, Val metrics: {'r2': -0.12398467238493316, 'mae': 3.9716159209778246, 'rmse': 4.91507878979274}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:50<00:00,  7.36s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 46, Train loss: 4.26085083138842, Val metrics: {'r2': -0.8080014956489718, 'mae': 5.069708621884158, 'rmse': 6.233754321975716}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:58<00:00,  7.88s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 47, Train loss: 4.326701990153411, Val metrics: {'r2': -0.1703414847466147, 'mae': 4.0669306806667525, 'rmse': 5.015411707992788}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:57<00:00,  7.83s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 48, Train loss: 4.331350214382506, Val metrics: {'r2': -0.08760652028973936, 'mae': 3.981887246547895, 'rmse': 4.834885470479462}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:33<00:00,  6.25s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 49, Train loss: 4.258983815578459, Val metrics: {'r2': -0.2106215772781388, 'mae': 4.07636214070266, 'rmse': 5.100990265369084}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:32<00:00,  6.15s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 50, Train loss: 4.216626627052069, Val metrics: {'r2': -0.2331198226546034, 'mae': 4.17919683530877, 'rmse': 5.1481705893883545}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:28<00:00,  5.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 51, Train loss: 4.26032984165574, Val metrics: {'r2': -0.15670880544798038, 'mae': 4.052162404369336, 'rmse': 4.986115224878665}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:25<00:00,  5.67s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 52, Train loss: 4.283942710052925, Val metrics: {'r2': -0.20085382701611287, 'mae': 4.155586292461786, 'rmse': 5.080370234489648}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:20<00:00,  5.37s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 53, Train loss: 4.184705567267154, Val metrics: {'r2': -0.17470684586731955, 'mae': 4.062971960805461, 'rmse': 5.024756717707202}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:24<00:00,  5.65s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 54, Train loss: 4.211033176643231, Val metrics: {'r2': -0.07780769478824245, 'mae': 3.940925292568201, 'rmse': 4.8130561641419325}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:40<00:00,  6.71s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 55, Train loss: 4.158264357020323, Val metrics: {'r2': -0.30157809623544596, 'mae': 4.242878874063332, 'rmse': 5.2891441733875}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:22<00:00,  5.50s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 56, Train loss: 4.22932331011501, Val metrics: {'r2': -0.17208402404654466, 'mae': 4.096727789618925, 'rmse': 5.019144080734773}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:27<00:00,  5.81s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 57, Train loss: 4.1674913897796495, Val metrics: {'r2': -0.15866994355186415, 'mae': 4.0873257179776274, 'rmse': 4.9903402810497}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:23<00:00,  5.58s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 58, Train loss: 4.137263328648727, Val metrics: {'r2': -0.19572075018873147, 'mae': 4.133130574795749, 'rmse': 5.069500527646879}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:23<00:00,  5.58s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 59, Train loss: 4.12029093735077, Val metrics: {'r2': -0.1416759964383214, 'mae': 4.022453243910032, 'rmse': 4.95360901138257}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:27<00:00,  5.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 60, Train loss: 4.110639866384007, Val metrics: {'r2': -0.29971245063179874, 'mae': 4.304066843849545, 'rmse': 5.285352158467635}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:25<00:00,  5.68s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 61, Train loss: 4.157814510329592, Val metrics: {'r2': -0.18720890034087212, 'mae': 4.032032034221138, 'rmse': 5.051424444606956}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:25<00:00,  5.71s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 62, Train loss: 4.164299700478652, Val metrics: {'r2': -0.3131432240351777, 'mae': 4.351946542194548, 'rmse': 5.31259046088985}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:27<00:00,  5.81s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 63, Train loss: 4.0668229347872895, Val metrics: {'r2': -0.3031560972227143, 'mae': 4.23848266374611, 'rmse': 5.2923494157697135}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:24<00:00,  5.64s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 64, Train loss: 4.06214583820582, Val metrics: {'r2': -0.2215699045357895, 'mae': 4.140089860994496, 'rmse': 5.124003903542754}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:20<00:00,  5.35s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 65, Train loss: 4.07531882855103, Val metrics: {'r2': -0.2854358613763046, 'mae': 4.3232586307532, 'rmse': 5.256243735271844}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:23<00:00,  5.59s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 66, Train loss: 4.121706545440907, Val metrics: {'r2': -0.2677078580451653, 'mae': 4.235548306371501, 'rmse': 5.219872328195458}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:22<00:00,  5.52s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 67, Train loss: 3.9975435851836836, Val metrics: {'r2': -0.5224394672166193, 'mae': 4.678616220615033, 'rmse': 5.720319766677568}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:21<00:00,  5.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 68, Train loss: 4.020782251510303, Val metrics: {'r2': -0.1817342910563262, 'mae': 4.0377003896212535, 'rmse': 5.0397640999307285}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:24<00:00,  5.67s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 69, Train loss: 4.004212735116074, Val metrics: {'r2': -0.20233457216005757, 'mae': 4.134495854951099, 'rmse': 5.08350151316252}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:31<00:00,  6.10s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 70, Train loss: 4.048604291956584, Val metrics: {'r2': -0.18875303552662825, 'mae': 4.04602473243205, 'rmse': 5.054708427563934}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:33<00:00,  6.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 71, Train loss: 3.9804544966444464, Val metrics: {'r2': -0.18462589118275785, 'mae': 4.08850092301786, 'rmse': 5.0459262629509904}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:27<00:00,  5.80s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 72, Train loss: 3.9836880857794457, Val metrics: {'r2': -0.2652790509546483, 'mae': 4.258281841967691, 'rmse': 5.214869542594994}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:26<00:00,  5.77s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 73, Train loss: 3.9530914897296823, Val metrics: {'r2': -0.2710435858260527, 'mae': 4.234431798321133, 'rmse': 5.2267353581702}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:26<00:00,  5.77s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 74, Train loss: 3.9325326282706468, Val metrics: {'r2': -0.27840064119493757, 'mae': 4.234604977000613, 'rmse': 5.241840228822739}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:26<00:00,  5.74s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 75, Train loss: 3.9023714183273532, Val metrics: {'r2': -0.20431677239305057, 'mae': 4.091411450172792, 'rmse': 5.087690184283775}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:31<00:00,  6.08s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 76, Train loss: 3.9605477958273827, Val metrics: {'r2': -0.2496120886038522, 'mae': 4.1863018015303775, 'rmse': 5.182483147998682}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:31<00:00,  6.11s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 77, Train loss: 3.9242692218280455, Val metrics: {'r2': -0.4254917272641865, 'mae': 4.517656710801638, 'rmse': 5.535191367144794}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:25<00:00,  5.72s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 78, Train loss: 3.937507546413477, Val metrics: {'r2': -0.27926408375038303, 'mae': 4.219971434100119, 'rmse': 5.243610121532906}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:25<00:00,  5.71s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 79, Train loss: 3.9467526603093743, Val metrics: {'r2': -0.2466836359177007, 'mae': 4.098449035119278, 'rmse': 5.176407038919848}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:26<00:00,  5.80s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 80, Train loss: 3.9241841638486497, Val metrics: {'r2': -0.45377024606880134, 'mae': 4.608100738167842, 'rmse': 5.589824561700201}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:30<00:00,  6.00s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 81, Train loss: 3.8926420092790637, Val metrics: {'r2': -0.29129718583449793, 'mae': 4.160522633118078, 'rmse': 5.268213803386745}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:27<00:00,  5.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 82, Train loss: 3.888965609888414, Val metrics: {'r2': -0.3087966325213152, 'mae': 4.275515562522866, 'rmse': 5.303790658775478}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:27<00:00,  5.81s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 83, Train loss: 3.869885732995477, Val metrics: {'r2': -0.3019345722016844, 'mae': 4.233868093496971, 'rmse': 5.2898684187117135}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:26<00:00,  5.79s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 84, Train loss: 3.8483633209391277, Val metrics: {'r2': -0.3292338617957593, 'mae': 4.268975064534701, 'rmse': 5.345040346148192}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:27<00:00,  5.82s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 85, Train loss: 3.826165046401429, Val metrics: {'r2': -0.36597734754632594, 'mae': 4.447779515487158, 'rmse': 5.418412173269794}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:28<00:00,  5.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 86, Train loss: 3.883228970520867, Val metrics: {'r2': -0.2342537788828376, 'mae': 4.108266432335477, 'rmse': 5.150537130955393}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:37<00:00,  6.48s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 87, Train loss: 3.8507297565580996, Val metrics: {'r2': -0.27608117865410864, 'mae': 4.212695637996307, 'rmse': 5.237082811040383}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:36<00:00,  6.41s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 88, Train loss: 3.8222668486026636, Val metrics: {'r2': -0.41626257955927937, 'mae': 4.43618414702221, 'rmse': 5.517243857411724}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:49<00:00,  7.33s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 89, Train loss: 3.841533944924438, Val metrics: {'r2': -0.4560549648881398, 'mae': 4.5612969002647255, 'rmse': 5.5942152704827866}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:33<00:00,  6.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 90, Train loss: 3.811834711365039, Val metrics: {'r2': -0.2563977875631456, 'mae': 4.1926795243420605, 'rmse': 5.196535172210701}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:26<00:00,  5.79s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 91, Train loss: 3.791967873283477, Val metrics: {'r2': -0.2784787596082514, 'mae': 4.259202498543955, 'rmse': 5.2420003812714295}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:27<00:00,  5.81s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 92, Train loss: 3.8164099472633994, Val metrics: {'r2': -0.209622417090902, 'mae': 4.073354303319213, 'rmse': 5.098884835178327}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:28<00:00,  5.89s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 93, Train loss: 3.781666653053594, Val metrics: {'r2': -0.3505695854023496, 'mae': 4.3178180486939315, 'rmse': 5.387766582537362}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:24<00:00,  5.62s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 94, Train loss: 3.7905406697906776, Val metrics: {'r2': -0.4375633254523732, 'mae': 4.478349800430781, 'rmse': 5.558578995208822}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:28<00:00,  5.88s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 95, Train loss: 3.7730018076569736, Val metrics: {'r2': -0.49671453434268953, 'mae': 4.567733925855709, 'rmse': 5.671785234229248}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:27<00:00,  5.83s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 96, Train loss: 3.731367285517451, Val metrics: {'r2': -0.38971818243885115, 'mae': 4.369265783398488, 'rmse': 5.465295637354715}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:24<00:00,  5.64s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 97, Train loss: 3.8405006058853006, Val metrics: {'r2': -0.6192482251057834, 'mae': 4.816409653166413, 'rmse': 5.899388597925337}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:24<00:00,  5.63s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 98, Train loss: 3.775286980192245, Val metrics: {'r2': -0.3596426412682714, 'mae': 4.3693213158236714, 'rmse': 5.40583365702527}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:31<00:00,  6.08s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 99, Train loss: 3.7438780776161678, Val metrics: {'r2': -0.3014380567399275, 'mae': 4.253221229982917, 'rmse': 5.288859630720391}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [01:22<00:00,  5.51s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 100, Train loss: 3.7576809136096254, Val metrics: {'r2': -0.3278560332766518, 'mae': 4.28189758325945, 'rmse': 5.342269404678653}\n",
            "Best Val metrics: {'r2': 0.030862673914101357, 'mae': 3.6827431724003015, 'rmse': 4.56397177503084}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "state_dict = None\n",
        "best_val_metric = -math.inf if higher_is_better else math.inf\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train()\n",
        "    val_pred = test(loader_dict[\"val\"])\n",
        "    #val_metrics = task.evaluate(val_pred, val_table)\n",
        "    val_metrics = custom_evaluate(val_pred, val_table, task.metrics)\n",
        "    print(f\"Epoch: {epoch:02d}, Train loss: {train_loss}, Val metrics: {val_metrics}\")\n",
        "\n",
        "    if (higher_is_better and val_metrics[tune_metric] > best_val_metric) or (\n",
        "        not higher_is_better and val_metrics[tune_metric] < best_val_metric\n",
        "    ):\n",
        "        best_val_metric = val_metrics[tune_metric]\n",
        "        state_dict = copy.deepcopy(model.state_dict())\n",
        "\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "val_pred = test(loader_dict[\"val\"])\n",
        "val_metrics = custom_evaluate(val_pred, val_table, task.metrics)\n",
        "print(f\"Best Val metrics: {val_metrics}\")\n",
        "\n",
        "#test_pred = test(loader_dict[\"test\"])\n",
        "#test_metrics = custom_evaluate(test_pred, test_table, task.metrics)\n",
        "#print(f\"Best test metrics: {test_metrics}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
